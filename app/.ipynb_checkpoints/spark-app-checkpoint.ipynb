{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "Click <a href=\"javascript:code_toggle()\">here</a> to show internal code."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "Click <a href=\"javascript:code_toggle()\">here</a> to show internal code.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    // AUTORUN ALL CELLS ON NOTEBOOK-LOAD!\n",
       "    require(\n",
       "        ['base/js/namespace', 'jquery'], \n",
       "        function(jupyter, $) {\n",
       "            $(jupyter.events).on(\"kernel_ready.Kernel\", function () {\n",
       "                console.log(\"Auto-running all cells-below...\");\n",
       "                jupyter.actions.call('jupyter-notebook:run-all-cells-below');\n",
       "                jupyter.actions.call('jupyter-notebook:save-notebook');\n",
       "            });\n",
       "        }\n",
       "    );\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "    // AUTORUN ALL CELLS ON NOTEBOOK-LOAD!\n",
    "    require(\n",
    "        ['base/js/namespace', 'jquery'], \n",
    "        function(jupyter, $) {\n",
    "            $(jupyter.events).on(\"kernel_ready.Kernel\", function () {\n",
    "                console.log(\"Auto-running all cells-below...\");\n",
    "                jupyter.actions.call('jupyter-notebook:run-all-cells-below');\n",
    "                jupyter.actions.call('jupyter-notebook:save-notebook');\n",
    "            });\n",
    "        }\n",
    "    );\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Twitter Analysis:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#format text for clear output\n",
    "def formatText(text):\n",
    "    formatted_text = ' '.join(word for word in text.split() if (word[0]!='#' and \n",
    "                                                                word[0]!=\"@\" and \n",
    "                                                                not word.startswith(\"http\") and \n",
    "                                                                not word.startswith(\"RT\")))\n",
    "    return formatted_text\n",
    "\n",
    "from IPython.display import display_html\n",
    "def restartKernel():\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "def sparkHashtag(): \n",
    "    %matplotlib inline\n",
    "    #define error variable\n",
    "    try:\n",
    "        ploterror\n",
    "    except UnboundLocalError:\n",
    "        ploterror = 0\n",
    "    #if try is successful it plot top 5 hashtags and calls sentiment() for sentiment analysis\n",
    "    try:\n",
    "        print(\"Gather Data for hashtag count...\")\n",
    "        time.sleep(5)\n",
    "        #top 5 hashtags:\n",
    "        #sqlContext.setConf('spark.sql.shuffle.partitions', '800')\n",
    "        top_5_tags = sqlContext.sql('Select hashtag, count from tweets') #gets temporary sql table and stores it in file\n",
    "        top_5_df = top_5_tags.toPandas() #convert file to pandas dataFrame\n",
    "        print(\"...successful\")\n",
    "        print(\"\")\n",
    "        plt.figure( figsize = (10, 8))\n",
    "        sns.barplot( x=\"count\", y=\"hashtag\", data=top_5_df)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "    #if try is not successful set error variable to +1 and try again. If error exceeds 5 then exit()\n",
    "    except:\n",
    "        print(\"Data was not ready....Waiting....\")\n",
    "        time.sleep(30)\n",
    "        if(ploterror >= 5):\n",
    "            print(\"Error on Streaming\")\n",
    "            print(\"Exiting...\")\n",
    "            ssc.stop()\n",
    "            f = open(\"var.py\", \"w\") #opens another file to store search variable \n",
    "            f.write(\"exit = True\") #overwrite the file\n",
    "            f.close()\n",
    "            file.kill()\n",
    "            os.system('taskkill /F /PID %d' %kill.id)\n",
    "            \n",
    "            sys.exit()\n",
    "        ploterror += 1\n",
    "        sparkHashtag()\n",
    "    sentiAnalysis()#if no error do sentiment analysis\n",
    "    \n",
    "######################################################################################################\n",
    "    \n",
    "def sentiAnalysis():\n",
    "    %matplotlib inline\n",
    "    #define error variable\n",
    "    try:\n",
    "        sentierror\n",
    "    except UnboundLocalError:\n",
    "        sentierror = 0\n",
    "    #if try is successful it does sentiment analysis on the twitter text, prints 5 tweets and plots the polarity count if the overall search topic is neutral/positive/negative\n",
    "    try:\n",
    "        print(\"Gather data for Sentiment Analysis...\")\n",
    "        #sentiment analysis:\n",
    "        #sqlContext.setConf('spark.sql.shuffle.partitions', '800')\n",
    "        senti = sqlContext.sql('Select text from sentiment') #gets temporary sql table and stores it in file\n",
    "        sen = senti.toPandas() #convert file to pandas dataFrame\n",
    "        print(\"...successful\")\n",
    "        sen['text'].replace('', np.nan, inplace=True) #if text empty replace it with NaN that dropna is recognizing it\n",
    "        sen.dropna(inplace=True) #drop columns which are empty\n",
    "        sen.reset_index(drop=True, inplace=True) #reset index for iteration\n",
    "        dfLen = len(sen) #get number of rows\n",
    "        i = 0 #count variable to get every tweet out of sen dataFrame\n",
    "        #make text clean\n",
    "        try:\n",
    "            while i < dfLen:\n",
    "                sen[\"text\"][i] = formatText(sen[\"text\"][i])\n",
    "                i += 1\n",
    "            print(\"Cleaning Text successful\")\n",
    "        except:\n",
    "            print(\"Error on formatting text\")\n",
    "        sen['text'].replace('', np.nan, inplace=True) #if text empty replace it with NaN that dropna is recognizing it\n",
    "        sen.dropna(inplace=True) #drop columns which are empty\n",
    "        sen.reset_index(drop=True, inplace=True) #reset index for iteration\n",
    "        dfLen = len(sen) #get number of rows\n",
    "        positiveList = [] #empty list for storing positive tweets\n",
    "        negativeList = [] #empty list for storing negative tweets\n",
    "        neutralList = [] #empty list for storing neutral tweets\n",
    "        countPosNegNeu = np.array([0, 0, 0]) #array for counting positive and negative tweets --> 0 is negative, 1 is positive, 2 is neutral\n",
    "        countNames = np.array([\"negative\", \"positive\", \"neutral\"]) #names for plotting\n",
    "        s = 0 #count variable for plotting 5 tweets with sentiment analysis\n",
    "        i = 0 #count variable to get every tweet out of sen dataFrame\n",
    "        print(\"analyse text for sentiment...\")\n",
    "        try:\n",
    "            #go through every item in sen[\"text\"], do the sentiment analysis and store in apropriate list\n",
    "            while i < dfLen:\n",
    "                opinion = TextBlob(sen[\"text\"][i]) #compute sentiment analysis on clean text and store it in opinion\n",
    "                #print 5 tweets with sentiment analysis\n",
    "                if(s <= 5):\n",
    "                    print(\"For the text: \", sen[\"text\"][i])\n",
    "                    print(\"The text polarity is: %.2f\" %opinion.sentiment[0])\n",
    "                    print(\"The text subjectivity is: %.2f\" %opinion.sentiment[1])\n",
    "                    print(\"*****************************************************\")\n",
    "                #store tweets in positive/negative/neutral polarity list\n",
    "                if(opinion.sentiment[0] > 0):\n",
    "                    countPosNegNeu[1] += 1 #add 1 to positive count because sentiment was positive\n",
    "                    positiveList.append(opinion.sentiment[0]) #stores tweet in positive list\n",
    "                elif(opinion.sentiment[0] < 0):\n",
    "                    countPosNegNeu[0] += 1 #add 1 to negative count because seintiment was negative\n",
    "                    negativeList.append(opinion.sentiment[0]) #stores tweet in negative list\n",
    "                else:\n",
    "                    countPosNegNeu[2] += 1 #add 1 to neutral count because seintiment was not negative or positive\n",
    "                    neutralList.append(opinion.sentiment[0]) #stores tweet in neutral list\n",
    "                i += 1 #count +1 for while loop\n",
    "                s += 1 #count +1 for text printing\n",
    "        except:\n",
    "            print(\"...not successful!\")\n",
    "        print(\"...done!\")\n",
    "        print(\"\")\n",
    "        #print positive/negative/neutral barplot\n",
    "        try:\n",
    "            print(\"Positive/Negative/Neutral Barplot\")\n",
    "            plt.figure(figsize = (10, 8))\n",
    "            sns.barplot(x=countNames, y=countPosNegNeu)\n",
    "            plt.show()\n",
    "            plt.clf() #cleans plot for new plotting\n",
    "        \n",
    "            #print positive/negative/neutral boxplot\n",
    "            boxplotdata = pd.DataFrame(list(zip(negativeList, positiveList)), columns =['negative', 'positive']) #do transformation for boxplot printing\n",
    "            print(\"Positive/Negative Boxplot\")\n",
    "            plt.figure(figsize = (10, 8))\n",
    "            sns.boxplot(data = boxplotdata)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "        except:\n",
    "            pass\n",
    "    #if try is not successful set error variable to +1 and try again. If error exceeds 5 then exit()\n",
    "    except:\n",
    "        print(\"Data was not ready for sentiment...waiting...\")\n",
    "        time.sleep(20)\n",
    "        if(sentierror >= 5):\n",
    "            print(\"Error on sentiment, getting not enough data!\")\n",
    "            print(\"Exiting...\")\n",
    "            ssc.stop()\n",
    "            f = open(\"var.py\", \"w\") #opens another file to store search variable \n",
    "            f.write(\"exit = True\") #overwrite the file\n",
    "            f.close()\n",
    "            file.kill()\n",
    "            os.system('taskkill /F /PID %d' %kill.id)\n",
    "            sys.exit()\n",
    "        sentierror += 1 \n",
    "        sentiAnalysis()\n",
    "    #if user wants to do another analysis on same search word execute plot() again. If not exit\n",
    "    time.sleep(5)\n",
    "    while True:\n",
    "        exit = input(\"Continue to get more Tweets?(y/n) : \")\n",
    "        if(exit == \"y\"):\n",
    "            sparkHashtag()\n",
    "        elif(exit == \"n\"):\n",
    "            again = input(\"\\nDo you want to use a new searchword?(y/n) : \")\n",
    "            if(again == \"y\"):\n",
    "                file.kill()\n",
    "                os.system('taskkill /F /PID %d' %kill.id)\n",
    "                print(\"\\033[1m\" + \"Please wait...\" + \"\\033[0m\")\n",
    "                restartKernel()\n",
    "            elif(again == \"n\"):\n",
    "                break\n",
    "            break\n",
    "    print(\"Exiting...\")\n",
    "    ssc.stop()\n",
    "    file.kill()\n",
    "    os.system('taskkill /F /PID %d' %kill.id)\n",
    "    sys.exit()\n",
    "    \n",
    "#####################################################################################################################\n",
    "    \n",
    "print(\"Starting Twitter Analysis:\\n\\n\")\n",
    "# import necessary libraries\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import subprocess\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "import IPython\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)\n",
    "\n",
    "findspark.init('C:\\spark\\spark') #initialise spark directory\n",
    "sc = SparkContext() #make new spark session\n",
    "ssc = StreamingContext(sc, 10) #we initiate the StreamingContext with 10 second batch interval\n",
    "sqlContext = SQLContext(sc) #next we initiate our sqlcontext\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555) #initiate streaming text from a TCP (socket) source:\n",
    "lines = socket_stream.window(60) #lines of tweets with socket_stream window of size 60, or 60 #seconds windows of time\n",
    "\n",
    "#create tuple for hashtag and counting and a named tupel Tweet\n",
    "hashtagfield = (\"hashtag\", \"count\")\n",
    "Tweet = namedtuple('Tweet', hashtagfield)\n",
    "\n",
    "#create tupel for sentiment analysis with text and count and a named tupel Sentiment\n",
    "sentifields = (\"text\", \"count\")\n",
    "Sentiment = namedtuple('Sentiment', sentifields)\n",
    "\n",
    "sentences = lines.flatMap( lambda text: text.split( \"\\n\" )) #Splits to a list containing of the single tweets \n",
    "table = sentences.map( lambda word: (word, 1))  #saves them in a tupel \n",
    "sent = table.map( lambda rec: Sentiment(rec[0], rec[1])) #stores the tupel in named tupel Sentiment\n",
    "sent.foreachRDD( lambda rdd: rdd.toDF().sort(desc(\"count\")).limit(10000).registerTempTable(\"sentiment\")) #save RDDs to a temporary sql table \n",
    "\n",
    "text = lines.flatMap(lambda text: text.split(\" \")) #Splits to a list\n",
    "hashtag = text.filter(lambda word: word.lower().startswith(\"#\"))  #Filter hashtag calls  \n",
    "lowerWord = hashtag.map(lambda word: (word.lower(), 1 )) #Lower cases the word\n",
    "getCount = lowerWord.reduceByKey(lambda a, b: a + b) #sum up the count of equal hashtags\n",
    "store = getCount.map(lambda rec: Tweet(rec[0], rec[1])) #stores the tupel in named tupel Tweet\n",
    "store.foreachRDD(lambda rdd: rdd.toDF().sort(desc(\"count\")).limit(5).registerTempTable(\"tweets\")) #Sorts RDDs by hashtag count, registers only top 5 hashtags and saves them to a temporary sql table\n",
    "\n",
    "#get user input for search\n",
    "while True:\n",
    "    search = input(\"Enter searchword: \")\n",
    "    if(search != \"\"):\n",
    "        print(\"Search for: \" + search)\n",
    "        break\n",
    "    print(\"You have to use a searchword!\")\n",
    "\n",
    "search = search.split(\",\") #splits user input by \",\"\n",
    "f = open(\"var.py\", \"w\") #opens another file to store search variable \n",
    "f.write(\"search = %s\\nexit = False\" % search) #overwrite the file\n",
    "f.close()\n",
    "file = subprocess.Popen(\"job.sh\", shell = True) #opens job.sh in shell command, which then calls the tweety api\n",
    "ssc.start() #starts streaming\n",
    "\n",
    "#has to wait 60 seconds to get enough tweets for RDDs because socket_stream.window(60) is set to 60 seconds\n",
    "print(\"Wait 60 seconds for data to stream properly...\")\n",
    "time.sleep(60)\n",
    "print(\"...successfull\")\n",
    "import kill\n",
    "sparkHashtag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
